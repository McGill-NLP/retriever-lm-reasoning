[2022-11-02 15:34:26,436][root][INFO] - CFG's local_rank=-1
[2022-11-02 15:34:26,437][root][INFO] - Env WORLD_SIZE=None
[2022-11-02 15:34:26,494][root][INFO] - Initialized host cn-a011 as d.rank -1 on device=cuda, n_gpu=1, world size=1
[2022-11-02 15:34:26,494][root][INFO] - 16-bits training: False 
[2022-11-02 15:34:26,494][root][INFO] - Reading saved model from /network/scratch/p/parishad.behnamghader/FiD/DPR/dpr/downloads/checkpoint/retriever/single/nq/bert-base-encoder.cp
[2022-11-02 15:34:27,543][root][INFO] - model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
[2022-11-02 15:34:27,544][root][INFO] - CFG:
[2022-11-02 15:34:27,546][root][INFO] - encoder:
  encoder_model_type: hf_bert
  pretrained_model_cfg: bert-base-uncased
  pretrained_file: null
  projection_dim: 0
  sequence_length: 256
  dropout: 0.1
  fix_ctx_encoder: false
  pretrained: true
ctx_sources:
  dpr_wiki:
    _target_: dpr.data.retriever_data.CsvCtxSrc
    file: data.wikipedia_split.psgs_w100
    id_prefix: 'wiki:'
model_file: /network/scratch/p/parishad.behnamghader/FiD/DPR/dpr/downloads/checkpoint/retriever/single/nq/bert-base-encoder.cp
ctx_src: data.retriever.nq-dev
encoder_type: ctx
out_file: my_out_file
do_lower_case: true
shard_id: 0
num_shards: 1
batch_size: 32
tables_as_passages: false
special_tokens: null
tables_chunk_sz: 100
tables_split_type: type1
local_rank: -1
device: cuda
distributed_world_size: 1
distributed_port: null
no_cuda: false
n_gpu: 1
fp16: false
fp16_opt_level: O1

[2022-11-02 15:34:27,557][transformers.file_utils][INFO] - PyTorch version 1.6.0 available.
[2022-11-02 15:34:27,917][dpr.models.hf_models][INFO] - Initializing HF BERT Encoder. cfg_name=bert-base-uncased
[2022-11-02 15:34:28,083][transformers.configuration_utils][INFO] - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/mila/p/parishad.behnamghader/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2022-11-02 15:34:28,084][transformers.configuration_utils][INFO] - Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2022-11-02 15:34:28,175][transformers.modeling_utils][INFO] - loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/mila/p/parishad.behnamghader/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2022-11-02 15:34:31,571][transformers.modeling_utils][INFO] - All model checkpoint weights were used when initializing HFBertEncoder.

[2022-11-02 15:34:31,571][transformers.modeling_utils][INFO] - All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
[2022-11-02 15:34:31,572][dpr.models.hf_models][INFO] - Initializing HF BERT Encoder. cfg_name=bert-base-uncased
[2022-11-02 15:34:31,711][transformers.configuration_utils][INFO] - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/mila/p/parishad.behnamghader/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2022-11-02 15:34:31,712][transformers.configuration_utils][INFO] - Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2022-11-02 15:34:31,756][transformers.modeling_utils][INFO] - loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/mila/p/parishad.behnamghader/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2022-11-02 15:34:35,061][transformers.modeling_utils][INFO] - All model checkpoint weights were used when initializing HFBertEncoder.

[2022-11-02 15:34:35,062][transformers.modeling_utils][INFO] - All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
[2022-11-02 15:34:35,201][transformers.tokenization_utils_base][INFO] - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/mila/p/parishad.behnamghader/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2022-11-02 15:34:40,193][root][INFO] - Loading saved model state ...
[2022-11-02 15:34:40,393][root][INFO] - reading data source: data.retriever.nq-dev
